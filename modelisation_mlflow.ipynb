{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import shap\n",
    "import time\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import  PCA\n",
    "from sklearn.model_selection import  KFold\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import f1_score,confusion_matrix , classification_report,balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, recall_score,roc_auc_score,accuracy_score,precision_score\n",
    "from business_score import custom_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',800)\n",
    "pd.set_option('display.max_columns',900)\n",
    "pd.options.display.max_colwidth  = 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"./data_csv/dataset_train_test/\"\n",
    "csv_train = 'data_train.csv'\n",
    "csv_test = 'data_test.csv'\n",
    "train = pd.read_csv(path + csv_train)\n",
    "df_test= pd.read_csv(path + csv_test)\n",
    "test = df_test.copy()\n",
    "train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['TARGET']\n",
    "X_train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "y = y_train.copy()\n",
    "X = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traitement de fichier Train:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1-Messing_values:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X.isna().sum()/X.shape[0]).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je fixe un threshold a 0.60 et je supprime les colonne avec des données manquentes >a ce threshold:\n",
    "threshold = 0.60\n",
    "print(f\"on a {X.shape} colonne dans le jeux d'entrainement\\n\")\n",
    "messing_val = X.columns[(X.isna().sum() / X.shape[0] > threshold)]\n",
    "print(F\"ON A {len(messing_val)} colonne avec des données manquantes supérieur ou égal à 60%\\n\")\n",
    "X.drop(columns = messing_val, axis = 1, inplace = True)\n",
    "print(f\"Apres la suppression le shape de X_train =  {X.shape} colonne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2-La distribution des données apreés l'imputation par mean et median:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualisation la distributio apres les impultatio n par le mean et mediane \n",
    "\n",
    "# impultation par median\n",
    "X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE_median'] = X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE'].fillna(X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE'].median())\n",
    "# impulation par mean\n",
    "X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE_mean'] = X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE'].fillna(X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE'].mean())\n",
    "\n",
    "sns.set(style=\"ticks\", rc = {'figure.figsize':(20, 4)})\n",
    "# visualisation\n",
    "plt.subplot(1, 3, 1)\n",
    "X[\"PREV_NAME_CLIENT_TYPE_Refreshed_MODE\"].plot(kind='kde',color='blue')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE_median'].plot(kind='kde',color='yellow')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "X['PREV_NAME_CLIENT_TYPE_Refreshed_MODE_mean'].plot(kind='kde',color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##--------------------------------------------------------\n",
    "\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_median'] = X['PREV_NAME_CLIENT_TYPE_New_MODE'].fillna(X['PREV_NAME_CLIENT_TYPE_New_MODE'].median())\n",
    "# impulation par mean\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_mean'] = X['PREV_NAME_CLIENT_TYPE_New_MODE'].fillna(X['PREV_NAME_CLIENT_TYPE_New_MODE'].mean())\n",
    "\n",
    "sns.set(style=\"ticks\", rc = {'figure.figsize':(20, 4)})\n",
    "# visualisation\n",
    "plt.subplot(1, 3, 1)\n",
    "X[\"PREV_NAME_CLIENT_TYPE_New_MODE\"].plot(kind='kde',color='blue')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_median'].plot(kind='kde',color='yellow')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_mean'].plot(kind='kde',color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##-------------------------------------------------------------\n",
    "\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_median'] = X['PREV_NAME_CLIENT_TYPE_New_MODE'].fillna(X['PREV_NAME_CLIENT_TYPE_New_MODE'].median())\n",
    "# impulation par mean\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_mean'] = X['PREV_NAME_CLIENT_TYPE_New_MODE'].fillna(X['PREV_NAME_CLIENT_TYPE_New_MODE'].mean())\n",
    "\n",
    "sns.set(style=\"ticks\", rc = {'figure.figsize':(20, 4)})\n",
    "# visualisation\n",
    "plt.subplot(1, 3, 1)\n",
    "X[\"PREV_NAME_CLIENT_TYPE_New_MODE\"].plot(kind='kde',color='blue')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_median'].plot(kind='kde',color='yellow')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "X['PREV_NAME_CLIENT_TYPE_New_MODE_mean'].plot(kind='kde',color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "X['PREV_NAME_PAYMENT_TYPE_Cash_MODE_median'] = X['PREV_NAME_PAYMENT_TYPE_Cash_MODE'].fillna(X['PREV_NAME_PAYMENT_TYPE_Cash_MODE'].median())\n",
    "# impulation par mean\n",
    "X['PREV_NAME_PAYMENT_TYPE_Cash_MODE_mean'] = X['PREV_NAME_PAYMENT_TYPE_Cash_MODE'].fillna(X['PREV_NAME_PAYMENT_TYPE_Cash_MODE'].mean())\n",
    "\n",
    "sns.set(style=\"ticks\", rc = {'figure.figsize':(20, 4)})\n",
    "# visualisation\n",
    "plt.subplot(1, 3, 1)\n",
    "X[\"PREV_NAME_PAYMENT_TYPE_Cash_MODE\"].plot(kind='kde',color='blue')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "X['PREV_NAME_PAYMENT_TYPE_Cash_MODE_median'].plot(kind='kde',color='yellow')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "X['PREV_NAME_PAYMENT_TYPE_Cash_MODE_mean'].plot(kind='kde',color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes qui cha,ge de distribution apres l'essaies d'imputation :\n",
    "col_to_drop = ['POS_MONTHS_BALANCE_MIN','POS_MONTHS_BALANCE_MAX','POS_SK_DPD_MEAN','POS_SK_DPD_MAX','POS_SK_DPD_DEF_MAX','POS_CNT_INSTALMENT_MEAN',\n",
    "               'POS_NAME_CONTRACT_STATUS_MODE',\"PREV_NAME_CONTRACT_STATUS_MODE\",'PREV_WEEKDAY_APPR_PROCESS_START_MODE',\n",
    "                'PREV_SELLERPLACE_AREA_MEAN','PREV_DAYS_DECISION_MAX','PREV_DAYS_DECISION_MIN','PREV_DAYS_DECISION_MEAN','PREV_HOUR_APPR_PROCESS_START_MEAN',\n",
    "                'INSTA_DAYS_ENTRY_PAYMENT_MIN','INSTA_AMT_INSTALMENT_MAX','INSTA_NEW_DBD_MEAN','INSTA_NUM_INSTALMENT_NUMBER_COUNT',\n",
    "                'INSTA_NUM_INSTALMENT_NUMBER_MEAN','PREV_FLAG_LAST_APPL_PER_CONTRACT_Y_MODE','PREV_FLAG_LAST_APPL_PER_CONTRACT_N_MODE',\n",
    "                'PREV_NAME_CONTRACT_TYPE_Consumer loans_MODE']\n",
    "X.drop(columns = col_to_drop,axis = 1,inplace = True)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation des colonnes avec mean dan sle fichier train:\n",
    "impute_with_mean = ['INSTA_DAYS_ENTRY_PAYMENT_MEAN','INSTA_NEW_DPD_MAX','INSTA_NEW_DBD_MAX','BURO_CREDIT_ACTIVE_MODE','BURO_CREDIT_TYPE_MODE','POS_CNT_INSTALMENT_MAX',\n",
    "                    'POS_CNT_INSTALMENT_MIN','PREV_NAME_CONTRACT_TYPE_Revolving loans_MODE']\n",
    "\n",
    "X[impute_with_mean] = X[impute_with_mean].fillna(X[impute_with_mean].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation des colonnes avec median dans le fichier train:\n",
    "columns_with_missing_values = X.columns[X.isna().any()].tolist()\n",
    "X[columns_with_missing_values] = X[columns_with_missing_values].fillna(X[columns_with_missing_values].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Traitement pour le fichier Test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.60\n",
    "print(f\"on a {test.shape} colonne dans le jeux de test \\n\")\n",
    "messing_val = test.columns[(test.isna().sum() / test.shape[0] > threshold)]\n",
    "print(F\"ON A {len(messing_val)} colonne avec des données manquantes supérieur ou égal à 60%\\n\")\n",
    "test.drop(columns = messing_val, axis = 1, inplace = True)\n",
    "print(f\"Apres la suppression le shape de test =  {test.shape} colonne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['POS_MONTHS_BALANCE_MIN','POS_MONTHS_BALANCE_MAX','POS_SK_DPD_MEAN','POS_SK_DPD_MAX','POS_SK_DPD_DEF_MAX','POS_CNT_INSTALMENT_MEAN',\n",
    "               'POS_NAME_CONTRACT_STATUS_MODE',\"PREV_NAME_CONTRACT_STATUS_MODE\",'PREV_WEEKDAY_APPR_PROCESS_START_MODE',\n",
    "                'PREV_SELLERPLACE_AREA_MEAN','PREV_DAYS_DECISION_MAX','PREV_DAYS_DECISION_MIN','PREV_DAYS_DECISION_MEAN','PREV_HOUR_APPR_PROCESS_START_MEAN',\n",
    "                'INSTA_DAYS_ENTRY_PAYMENT_MIN','INSTA_AMT_INSTALMENT_MAX','INSTA_NEW_DBD_MEAN','INSTA_NUM_INSTALMENT_NUMBER_COUNT',\n",
    "                'INSTA_NUM_INSTALMENT_NUMBER_MEAN','PREV_FLAG_LAST_APPL_PER_CONTRACT_Y_MODE','PREV_FLAG_LAST_APPL_PER_CONTRACT_N_MODE',\n",
    "                'PREV_NAME_CONTRACT_TYPE_Consumer loans_MODE']\n",
    "test.drop(columns = col_to_drop,axis = 1,inplace = True)    \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_with_mean = ['INSTA_DAYS_ENTRY_PAYMENT_MEAN','INSTA_NEW_DPD_MAX','INSTA_NEW_DBD_MAX','BURO_CREDIT_ACTIVE_MODE','BURO_CREDIT_TYPE_MODE','POS_CNT_INSTALMENT_MAX',\n",
    "                    'POS_CNT_INSTALMENT_MIN','PREV_NAME_CONTRACT_TYPE_Revolving loans_MODE']\n",
    "\n",
    "test[impute_with_mean] = test[impute_with_mean].fillna(test[impute_with_mean].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_values = test.columns[test.isna().any()].tolist()\n",
    "test[columns_with_missing_values] = test[columns_with_missing_values].fillna(test[columns_with_missing_values].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messing_values = (test.isna().sum()/test.shape[0]).sort_values(ascending = False)*100\n",
    "messing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_x = X.columns.to_list()\n",
    "liste_test = test.columns.to_list()\n",
    "set_test_train = set(liste_test) - set(X)\n",
    "list_to_drop = list(set_test_train)\n",
    "list_to_drop.remove(\"SK_ID_CURR\")\n",
    "#list_to_drop\n",
    "test.drop(columns = list_to_drop, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messing_values = (test.isna().sum()/test.shape[0]).sort_values(ascending = False)*100\n",
    "messing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"./models/data/test.joblib\"\n",
    "# Enregistrez le modèle\n",
    "joblib.dump(test, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- La fonction cout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On va essaies de diminuer les FN(C'est des clients qui n'ont pas remboursé leurs credit et predit comme remboursé , ce type d'erreur est tres dangereux ).\n",
    "\n",
    "* TN: c'est des clients qui porte en fait rien pour l'entreprise.\n",
    "* cette fonction permet alors de trouver un score metier efficace.\n",
    "\n",
    "$$10*FN + FP  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 33\n",
    "X_train ,X_test, y_train,y_test = train_test_split(X , y,test_size = 0.3,stratify = y,random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nle shape de X_train  = {X_train.shape}\\n')\n",
    "print(f'Le shape de X_test = {X_test.shape}\\n')\n",
    "print(f'le shape de y_test = {y_test.shape}\\n')\n",
    "print(f'le shape de y_train = {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.pie(y.value_counts(normalize=True), autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standardisation des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('standardization', StandardScaler()),  \n",
    "])\n",
    "pipeline.fit(X_train)\n",
    "X_train_scaler = pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaler = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scorer = make_scorer(custom_score, greater_is_better=False)\n",
    "scoring = {\n",
    "        'AUC': 'roc_auc',\n",
    "        'business_score': custom_scorer,\n",
    "        'Accuracy':'accuracy',\n",
    "        'Precision': 'precision',\n",
    "        'Recall': 'recall',\n",
    "        'F1_Score': 'f1',\n",
    "        'Balanced_Accuracy': 'balanced_accuracy'\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -LogisticRegression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-Manual upsampling within folds:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_params = {\n",
    "        'solver': 'liblinear',\n",
    "    'C': 0.1 }\n",
    "    \n",
    "\n",
    "def score_model(model, params, cv=None):\n",
    "    \"\"\"\n",
    "    Creates folds manually, and upsamples within each fold.\n",
    "    Returns an array of validation (recall) scores\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    \n",
    "    if cv is None:\n",
    "        cv = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "    smoter = SMOTE(random_state=42)\n",
    "    \n",
    "    val_scores = []\n",
    "    train_scores = []\n",
    "    time_scores = []\n",
    "\n",
    "    for train_fold_index, val_fold_index in cv.split(X_train_scaler, y_train):\n",
    "        # Get the training data\n",
    "        X_train_fold, y_train_fold = X_train_scaler[train_fold_index], y_train.iloc[train_fold_index]\n",
    "        # Get the validation data\n",
    "        X_val_fold, y_val_fold = X_train_scaler[val_fold_index], y_train.iloc[val_fold_index]\n",
    "\n",
    "        # Upsample only the data in the training section\n",
    "        X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(X_train_fold,\n",
    "                                                                           y_train_fold)\n",
    "        # Fit the model on the upsampled training data\n",
    "        model_obj = model(**params).fit(X_train_fold_upsample, y_train_fold_upsample)\n",
    "        # Score the model on the (non-upsampled) validation data\n",
    "        score = roc_auc_score(y_val_fold, model_obj.predict_proba(X_val_fold)[:,1])\n",
    "        train_score = roc_auc_score(y_train_fold, model_obj.predict_proba(X_train_fold)[:,1])\n",
    "\n",
    "        val_scores.append(score)\n",
    "        train_scores.append(train_score)\n",
    "    return np.array(val_scores), np.array(train_scores)\n",
    "\n",
    "# Example of the model in action\n",
    "valid_score,train_score = score_model(LogisticRegression, example_params, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lg = {\n",
    "\n",
    "     'solver': ['liblinear'],\n",
    "    'C':[0.01,0.1,0.2] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_tracker = []\n",
    "for n in params_lg['solver']:\n",
    "    for m in params_lg['C']:\n",
    "        example_params = {\n",
    "            'solver': n,\n",
    "            'C': m,\n",
    "            'random_state': 13\n",
    "        }\n",
    "        example_params['AUC'] = score_model(LogisticRegression, \n",
    "                                               example_params, cv=kf)[0].mean()\n",
    "        score_tracker.append(example_params)\n",
    "        \n",
    "# What's the best model?\n",
    "sorted(score_tracker, key=lambda x: x['AUC'], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_resample(X_train_scaler, y_train)\n",
    "y_train_upsample.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl= LogisticRegression(solver='liblinear', C = 0.2, random_state=13)\n",
    "model_rl = rl.fit(X_train_upsample, y_train_upsample)\n",
    "roc_auc_score(y_test, model_rl.predict_proba(X_test_scaler)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Using imblearn pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(),\n",
    "                              LogisticRegression(random_state=13))\n",
    "new_params = {'logisticregression__' + key: params_lg[key] for key in params_lg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_imba = GridSearchCV(imba_pipeline, param_grid= new_params, cv=kf, scoring= scoring,refit='AUC',\n",
    "                        return_train_score=True)\n",
    "model_rl = grid_imba.fit(X_train_scaler, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_imba.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esemator = grid_imba.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_imba.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = grid_imba.predict_proba(X_test_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = grid_imba.predict_proba(X_test_scaler)\n",
    "roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- * Quelque soit la methode: manuelles ou bien on utilisant les pipeline de imblearn, on trouve les meme scores de prediction,\n",
    "le suréchantillonnage est trés bien gerer avec c'est deux methodes, avec c'est deux methodes utiliser on a éviter le data leakage(fuite de données)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Comparaison entre les modéles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model,X_train ,y_train, param_grid):\n",
    "    kf =  KFold(n_splits=5, random_state=42, shuffle= True)\n",
    "    imb_pipe  = make_pipeline(StandardScaler(),SMOTE(),\n",
    "                              model(random_state=13))\n",
    "    \n",
    "    \n",
    "    grid_imba = GridSearchCV(imb_pipe, param_grid= param_grid, cv=kf, scoring= scoring,refit='AUC',\n",
    "                        return_train_score=True)\n",
    "    grid_imba.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "    cv_results = grid_imba.cv_results_\n",
    "    ## Acceder aux score pour chaque métrique dans le dictionnaire de scoring:\n",
    "    all_scores = {}\n",
    "    for metric in scoring:\n",
    "        mean_scores = cv_results['mean_test_' + metric]\n",
    "        std_scores = cv_results['std_test_' + metric]\n",
    "        all_scores[metric] = {\n",
    "            'mean_score':mean_scores,\n",
    "            'std_score':std_scores\n",
    "        }\n",
    "    # meilleurs params:\n",
    "    best_params = grid_imba.best_params_\n",
    "\n",
    "\n",
    "    eval_dict = {}\n",
    "    test_scoring_dict ={}\n",
    "   \n",
    "    \n",
    "    for i in scoring :\n",
    "        test_scoring_dict[i] = (cv_results[\"mean_test_\" + i][grid_imba.best_index_])\n",
    "      \n",
    "\n",
    "    train_time = (cv_results[\"mean_fit_time\"][grid_imba.best_index_])\n",
    "\n",
    "    eval_dict = {'estimator' : grid_imba.best_estimator_,\n",
    "                    \n",
    "                    'best_grid_params' : best_params,\n",
    "                    'test_scores' : test_scoring_dict, \n",
    "                    'train_time' : train_time }\n",
    "\n",
    "    return eval_dict , cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction sur x_test et calcul des scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculat_score_for_prediction(estimator, X_test, y_test, scoring):\n",
    "\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    y_test_pred = estimator.predict_proba(X_test)\n",
    "    dict_score = {}\n",
    "    for score in scoring:\n",
    "        if score == 'business_score':\n",
    "            dict_score[score] = custom_score(y_test, y_pred)\n",
    "        elif score == 'AUC':\n",
    "            dict_score[score] = roc_auc_score(y_test, y_test_pred[:,1])\n",
    "        elif score == 'Accuracy':\n",
    "            dict_score[score] = accuracy_score(y_test, y_test_pred[:,1].round())\n",
    "        elif score == 'Precision':\n",
    "            dict_score[score] = precision_score(y_test, y_test_pred[:,1].round())\n",
    "        elif score == 'Recall':\n",
    "            dict_score[score] = recall_score(y_test, y_test_pred[:,1].round())\n",
    "        elif score == 'F1_Score':\n",
    "            dict_score[score] = f1_score(y_test, y_test_pred[:,1].round())\n",
    "        elif score =='Balanced_Accuracy':\n",
    "            dict_score[score] = balanced_accuracy_score(y_test, y_test_pred[:,1].round())\n",
    "    return dict_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_from_frame(name, validate_score, test_score):\n",
    "\n",
    "    # prefix test pour cle de dict: \n",
    "    test_score = {\"test_\" + key: value for key, value in test_score.items()}\n",
    "    # prefix pour validate pour cle de dict:\n",
    "    val_scores = validate_score['test_scores']\n",
    "    val_score = {\"val_\" + key: value for key, value in val_scores.items()}\n",
    "    time = validate_score['train_time']\n",
    "    # rassemblage des deux dictionnaire:\n",
    "    dict_metrics = {**test_score,**val_score}\n",
    "    \n",
    "    # transform en dataframe \n",
    "    df = pd.DataFrame.from_dict(dict_metrics, orient = 'index')\n",
    "    df = df.transpose()\n",
    "    df = df.assign(Models= name,Time=validate_score['train_time'])\n",
    "    model_col = df.pop(df.columns[-2])\n",
    "    df.insert(0, model_col.name, model_col)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Modèle DummyClassifier():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dc = {\n",
    "            'dummyclassifier__strategy': ['stratified']}\n",
    "eval_metric_dc, cv_results = evaluate_models(DummyClassifier,X_train ,y_train, param_grid_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = eval_metric_dc['estimator']\n",
    "scor_pred_dc = calculat_score_for_prediction(estimator, X_test, y_test, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dc =  metrics_from_frame('DummyClassifier', eval_metric_dc, scor_pred_dc )\n",
    "df_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Modèle RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "            'randomforestclassifier__n_estimators': [10,50,100],\n",
    "            'randomforestclassifier__max_depth':[None, 5, 10, 30] }\n",
    "eval_metric_rf, dict_rf = evaluate_models(RandomForestClassifier,X_train ,y_train, param_grid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = eval_metric_rf['estimator']\n",
    "rf_pred_test = calculat_score_for_prediction(estimator, X_test, y_test, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf =  metrics_from_frame('RandomForestClassifier', eval_metric_rf, rf_pred_test )\n",
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "\n",
    "            'logisticregression__solver': ['liblinear'],\n",
    "            'logisticregression__C':[0.01,0.1,0.2] }\n",
    "eval_metric_lr , metric_lr= evaluate_models(LogisticRegression,X_train ,y_train, param_grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = eval_metric_lr['estimator']\n",
    "        \n",
    "dict_score_pred = calculat_score_for_prediction(estimator, X_test, y_test, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logi =  metrics_from_frame('LogisticRegression', eval_metric_lr, dict_score_pred )\n",
    "df_logi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gb = {\n",
    "    \"gradientboostingclassifier__learning_rate\": [0.01, 0.1],\n",
    "    \"gradientboostingclassifier__n_estimators\": [10, 50, 100],\n",
    "    \"gradientboostingclassifier__max_depth\": [5]\n",
    "}\n",
    "\n",
    "eval_metric_gb , metrics_gb= evaluate_models(GradientBoostingClassifier,X_train ,y_train, param_grid_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = eval_metric_gb['estimator']     \n",
    "test_score_gb = calculat_score_for_prediction(estimator, X_test, y_test, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb =  metrics_from_frame('GradientBoostingClassifier', eval_metric_gb, test_score_gb)\n",
    "df_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgb = {\n",
    "    \n",
    "    'lgbmclassifier__learning_rate':[0.017,0.01 , 0.03,1],\n",
    "    'lgbmclassifier__n_estimators':[1000,15000],\n",
    "    'lgbmclassifier__num_leaves':[34,38,40],\n",
    "       'lgbmclassifier__max_depth':[5,10],\n",
    "        \n",
    "    \n",
    "            }\n",
    "\n",
    "eval_metric_lgb,dict_lgb = evaluate_models(LGBMClassifier,X_train ,y_train, param_grid_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = eval_metric_lgb['estimator']\n",
    "        \n",
    "test_score_lgb = calculat_score_for_prediction(estimator, X_test, y_test, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lgbm =  metrics_from_frame('LGBMClassifier', eval_metric_lgb, test_score_lgb)\n",
    "df_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_dc,df_rf])\n",
    "data = pd.concat([data,df_lgbm ])\n",
    "data = pd.concat([data, df_gb])\n",
    "data.sort_values(\"test_AUC\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation de modèle avec mlflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mlflow(model, run_name, results_, name):\n",
    "    \n",
    "    mlflow.set_experiment(name)\n",
    "    mlflow.sklearn.autolog() \n",
    "    for i, params in enumerate(results_['params']):\n",
    "        with mlflow.start_run(run_name=f\"{run_name}{i + 1}\",nested=True):\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "                #log model:\n",
    "                mlflow.sklearn.log_model(model(params), \"Classification_run\")\n",
    "            list_col = [col for col in results_ if col.startswith(\"mean_test_\")]\n",
    "            for col in list_col:\n",
    "                for i in results_[col]:\n",
    "                    mlflow.log_metric(col,i) \n",
    "            for time in results_['mean_score_time']:\n",
    "                mlflow.log_metric('mean_score_time',time) \n",
    "        \n",
    "        \n",
    "        # # Enregister les figures\n",
    "        # mlflow.log_artifact('confusion_matrix.png', artifact_path='images')\n",
    "        # mlflow.log_artifact('AUC_ROC.png',artifact_path='images' )\n",
    "        # Enregistrer le modèle\n",
    "       \n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- mlflow_lgbmclassifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mlflow(LGBMClassifier, 'lgbm',dict_lgb, 'LGBMClassifier' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- mlflow_Gradientboostingclassifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mlflow(GradientBoostingClassifier, 'g_boosting',metrics_gb, 'GradientBoostingClassifier' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3- mlflow_RandomForestclassifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mlflow(RandomForestClassifier, 'RandomForest',dict_rf, 'RandomForestClassifier' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4- mlflow_Dummyclassifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mlflow(DummyClassifier, 'Dummyclassifier',cv_results, 'Dummyclassifier' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models with MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mlflow(model, run_name, params, score_metrics):\n",
    "    \n",
    "    y_predict = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "  \n",
    "  \n",
    "    plt.figure(figsize =(8,5))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,y_proba)\n",
    "    auc = roc_auc_score(y_test,y_proba)\n",
    "    plt.plot(fpr,tpr,label = f'AUC Roc Curve with Area Under the curve = {auc}')\n",
    "    plt.legend()\n",
    "    plt.savefig('./img/AUC_ROC.png')\n",
    "\n",
    "\n",
    "\n",
    "    mlflow.set_experiment(run_name)\n",
    "    mlflow.sklearn.autolog() \n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "       \n",
    "        mlflow.log_params(params)\n",
    "\n",
    "      \n",
    "        for index, value in score_metrics.items():\n",
    "            mlflow.log_metric(f'test_{index}', value)\n",
    "\n",
    "        \n",
    "        mlflow.sklearn.log_model(model, \"Classification_run\")\n",
    "        mlflow.log_artifact('./img/AUC_ROC.png', \"AUC ROC curve\")\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mlflow_dummyclassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval_metric_dc['estimator']\n",
    "params = eval_metric_dc['best_grid_params']\n",
    "score_metrics = scor_pred_dc\n",
    "log_mlflow(model, 'best_model_dummyclassifier',params,score_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model Randomforest with mlflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval_metric_rf['estimator']\n",
    "params = eval_metric_rf['best_grid_params']\n",
    "score_metrics = rf_pred_test\n",
    "\n",
    "log_mlflow(model, 'best_model_Randomforestclassifier',params,score_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model gradienboosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval_metric_gb['estimator']\n",
    "params = eval_metric_gb['best_grid_params']\n",
    "score_metrics = scor_pred_dc\n",
    "\n",
    "log_mlflow(model, 'best_model_gradientbosting',params,score_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Best model LGBMlassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval_metric_lgb['estimator']\n",
    "params = eval_metric_lgb['best_grid_params']\n",
    "score_metrics =test_score_lgb\n",
    "\n",
    "log_mlflow(model, 'best_model_LGBMClassifier',params,score_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roc_auc de comparaison entre les modèles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "ns_fpr, ns_tpr,_ = roc_curve(y_test, ns_probs)\n",
    "\n",
    "### LGBMClassifier:\n",
    "y_proba = eval_metric_lgb['estimator'].predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_proba)\n",
    "auc = roc_auc_score(y_test,y_proba)\n",
    "\n",
    "### Randomforestclassifier:\n",
    "rf_probs = eval_metric_rf['estimator'].predict_proba(X_test)[:,1]\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "###dummyclassifier\n",
    "\n",
    "dc_probs = eval_metric_dc['estimator'].predict_proba(X_test)[:,1]\n",
    "dc_auc =roc_auc_score(y_test, dc_probs)\n",
    "dc_fpr, dc_tpr, _ = roc_curve(y_test, dc_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comparaison entre les score auc:\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No skill prediction (AUC = %0.3f)' % ns_auc)\n",
    "plt.plot(fpr, tpr, marker='.', label='LGBMClassifier (AUC = %0.3f)' % auc)\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Randomforestclassifier (AUC = %0.3f)' % rf_auc)\n",
    "plt.plot(dc_fpr, dc_tpr, marker='.', label='Dummy classifier (AUC = %0.3f)' % dc_auc)\n",
    "\n",
    "plt.title('Corbe roc_auc ')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de réference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = eval_metric_lgb['estimator'][2]\n",
    "model_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_lgbm.fit(X_train_scaler,np.ravel(y_train))\n",
    "model_filename = './models/LGBMClassifier.model'\n",
    "# Enregistrez le modèle\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretabilité locale et globale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.summary_plot(shap_values[1],\n",
    "                  features = X_test,\n",
    "                  feature_names = X_test.columns,\n",
    "                  plot_size = (10,8),\n",
    "                  show = False\n",
    "\n",
    "                  )\n",
    "plt.title(\"Interprétation Globale (shap-values)\",fontsize = 14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation locale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()  \n",
    "instance_idx = 0\n",
    "shap.force_plot(explainer.expected_value[1],shap_values[1][instance_idx,:], X_test.iloc[instance_idx, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement des modèles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### enregistement du model explainer:\n",
    "joblib.dump(explainer, './models/explainer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(learning_rate=0.03,num_leaves=34,n_estimators=1000)\n",
    "model = lgbm.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enregistement du modèle LGBMCassifier:\n",
    "model_filename = './models/LGBMClassifier.model'\n",
    "# Enregistrez le modèle\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### enregestrement de scaler:\n",
    "scaler_filename = './models/scaler.model'\n",
    "# Enregistrez le modèle\n",
    "joblib.dump(pipeline, scaler_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
